{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <FONT COLOR=\"#273B5F\"> Structured pruning of image classification keras models by using NetsPresso </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetsPresso is a platform that provides tools and infrastructure for building and deploying machine learning models. One of its powerful features is the NetsPresso Compressor, which helps optimizing machine learning models by reducing their size and computational requirements. This is achieved through techniques such as pruning, which can be done using various norms like L2 and GM, or filter decomposition techniques such as Tucker decomposition, Singular Value Decomposition (SVD), or Canonical Polyadic (CP).\n",
    "\n",
    "Structured pruning is a model compression technique that involves removing specific weights or connections from a neural network while preserving its overall structure. This technique can significantly reduce the size of the model and make it faster to run on edge devices without compromising its accuracy.\n",
    "\n",
    "**This notebook demonstrates the process of static structured pruning for deep learning models using NetsPresso. It covers the process of training a classification model, pruning, fine-tuning, and quantizing it using the STM32 model zoo. The STM32Cube.AI Developer Cloud is then used to benchmark the models.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License of the Jupyter Notebook\n",
    "\n",
    "Copyright (c) 2022 STMicroelectronics.\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "This software is licensed under terms that can be found in the LICENSE file in\n",
    "the root directory of this software component.\n",
    "\n",
    "If no LICENSE file comes with this software, it is provided AS-IS.\n",
    "\n",
    "Copyrights (c) 2024. Nota inc. All rights reserved.\n",
    "\n",
    "Any code statements to execute NetsPresso optimization process belongs to Nota inc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid #273B5F\">\n",
    "<h2>Table of content</h2>\n",
    "<ul style=\"list-style-type: none\">\n",
    "  <li><a href=\"#Setup\">1. Setup Instructions</a></li>\n",
    "\n",
    "<li><a href=\"#Prep\">2. Preparing the Baseline Model</a></li>\n",
    "    <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#Training\">2.1 Training, Quantization, and Evaluation</a></li>\n",
    "    <li><a href=\"#Benchmarking\">2.2 Benchmarking the Baseline Model</a></li>\n",
    "  </ul>\n",
    "<li><a href=\"#Pruning the Model with NetsPresso Compressor\">3. Pruning the Model with NetsPresso Compressor</a></li>\n",
    "<li><a href=\"#Fine-Tuning\">4. Fine-Tuning the Pruned Model</a></li>\n",
    "<li><a href=\"#Quantizing\">5. Quantizing the Pruned Model</a></li>\n",
    "<li><a href=\"#Evaluating\">6. Evaluating the Pruned Model</a></li>\n",
    "<li><a href=\"#Benchmarking-the-Pruned\">7. Benchmarking the Pruned Model on STM32Cube.AI Developer Cloud</a></li>        \n",
    "<li><a href=\"#Comparing\">8. Comparing NetsPresso Pruned Model with Baseline Model</a></li>    \n",
    "  </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Setup\">\n",
    "    <h2>1. Setup Instructions</h2>\n",
    "</div>\n",
    "\n",
    "In this notebook, we will apply structured pruning on an image classification model using [NetsPresso](https://netspresso.ai/) and train, quantize, and benchmark the model using the [STM32 model zoo](https://github.com/STMicroelectronics/stm32ai-modelzoo/tree/main). \n",
    "\n",
    "The STM32 model zoo valuable resource, accessible on GitHub, offers a range of use cases such as image classification, object detection, audio event detection, hand posture, and human activity recognition. It provides numerous services, including training, evaluation, prediction, deployment, quantization, benchmarking, and chained services, such as chain_tbqeb, chain_tqe, chain_eqe, chain_qb, chain_eqeb, and chain_qd, which are thoroughly explained in their respective readmes.\n",
    "\n",
    "To get started, you'll need to add the stm32ai model zoo repository as a submodule by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "!git clone https://github.com/STMicroelectronics/stm32ai-modelzoo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above, navigate to the stm32ai-modelzoo repository and install the required libraries by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('stm32ai-modelzoo')\n",
    "!pip install -r requirements.txt\n",
    "!pip install netspresso==1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be utilizing the various services of the image classification service. To do so, we must navigate to the image classification source by running the code section below and use the `stm32ai_main.py` script in conjunction with a YAML file in the next sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('image_classification/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Prep\">\n",
    "    <h2>2. Preparing the Baseline Model</h2>\n",
    "</div>\n",
    "\n",
    "<div id=\"Training\">\n",
    "    <h3>2.1 Training, Quantization, and Evaluation</h3>\n",
    "</div>\n",
    "\n",
    "In this section, we will be training, quantizing, and evaluating a classification model to consider it as a baseline model. We will use the MobileNetV2 model from the Model Zoo and the [tf_flowers](https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz) classification dataset as an example. To achieve this, we will be using the `chain_tqe_config.yaml` file to specify the service and the set of configuration parameters such as the model, the dataset, the number of epochs, and the preprocessing parameters, among others. Please feel free to review and adjust the training parameters as needed. If you have already trained your own model, you may proceed to the next section and skip this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path  ../../../image_classification/config_files  --config-name chain_tqe_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the training and evaluation artifacts are saved in the current output simulation directory, which is located at **experiments_outputs/baseline_training**. The trained model is also saved in **experiments_outputs/baseline_training/best_model** directory and is called `best_model.h5` which will be used in the next steps of the notebook. You can retrieve the confusion matrix generated after evaluating the float and the quantized model on the test set by navigating to the appropriate directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Benchmarking\">\n",
    "    <h3>2.2 Benchmarking the Baseline Model</h2>\n",
    "</div>\n",
    "\n",
    "In this section we use the [STM32Cube.AI Developer Cloud](https://stm32ai-cs.st.com/home) to benchmark the baseline model on the **STM32H747I-DISCO** board.\n",
    "\n",
    "If you are behind a proxy, you can uncomment and fill the following proxy settings.\n",
    "\n",
    "**NOTE** : If the password contains some special characters like `@`, `:` etc. they need to be url-encoded with their ASCII values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['http_proxy'] = \"http://user:passwd@ip_address:port\"\n",
    "# os.environ['https_proxy'] = \"https://user:passwd@ip_address:port\"\n",
    "# And eventually disable SSL verification\n",
    "# os.environ['NO_SSL_VERIFY'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Set environment variables with your credentials to access STM32Cube.AI Developer Cloud. If you don't have an account yet go to : https://stm32ai-cs.st.com/home and click on sign in to create an account. Then set the environment variables below with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "email ='xxx.yyy@st.com'\n",
    "os.environ['stmai_username'] = email\n",
    "print('Enter your password')\n",
    "password = getpass.getpass()\n",
    "os.environ['stmai_password'] = password\n",
    "os.environ['NO_SSL_VERIFY'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `benchmarking_config.yaml` file to measure the performance of the baseline model on the **STM32H747I-DISCO**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path  ../../../image_classification/config_files  --config-name baseline_benchmarking_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Pruning\">\n",
    "    <h2>3. Pruning the Model with NetsPresso Compressor</h2>\n",
    "</div>\n",
    "\n",
    "To use NetsPresso python package, first sign up for the [NetsPresso website](https://netspresso.ai). Then, please enter the email and password that you registered with in the code section below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from netspresso import NetsPresso\n",
    "\n",
    "#log to the NetsPresso server\n",
    "\n",
    "email ='xxx.yyy@st.com'\n",
    "print('Enter your password') \n",
    "password = getpass.getpass()\n",
    "\n",
    "# Creating an instance of NetsPresso using the email and password entered by the user for connecting to the NetsPresso service.\n",
    "netspresso = NetsPresso(email=email, password=password)\n",
    "\n",
    "# Creating an instance of Compressor using the created NetsPresso instance.\n",
    "compressor = netspresso.compressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your trained model, simply enter the required information.\n",
    "To perform advanced compression, please enter the required parameters. You can find a description of each parameter in the table below.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Option</th>\n",
    "<th style=\"text-align: left\">Description</th>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">compression_method\n",
    "   \n",
    "<td style=\"text-align: left\">The selected compression method, example: <strong>'PR_L2'</strong> for L2 Norm Pruning or <strong>'PR_GM'</strong> for GM pruning </td>\n",
    "</tr>  \n",
    " \n",
    "<tr>\n",
    "<td style=\"text-align: left\">recommendation_method</td>\n",
    "<td style=\"text-align: left\">The selected recommendation method, the method to consider rge layer wise importance for structured pruning, example: <strong>'SLAMP'</strong>\n",
    " for Structured Layer-adaptive Sparsity for the Magnitude-based Pruning or <strong>'VBMF'</strong> for Variational Bayesian Matrix Factorization </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">recommendation_ratio</td>\n",
    "<td style=\"text-align: left\">The compression ratio recommended by the recommendation method, refers to the amount of the filters considering layer wise importance  </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">input_model_path</td>\n",
    "<td style=\"text-align: left\">The file path where the model is located</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">output_dir</td>\n",
    "<td style=\"text-align: left\">The local path to save the compressed model</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">framework   \n",
    "<td style=\"text-align: left\">The framework of the model</td>\n",
    "</tr>  \n",
    "\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">input_shapes</td>\n",
    "<td style=\"text-align: left\">Input shapes of the model</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">options</td>\n",
    "<td style=\"text-align: left\">The options for pruning method</td>\n",
    "</tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netspresso.enums import Framework, CompressionMethod, RecommendationMethod\n",
    "\n",
    "# Set the path of the saved model file\n",
    "baseline_model_path = '../../../image_classification/experiments_outputs/baseline_training/saved_models/best_model.h5'\n",
    "\n",
    "# Set the path of the compressed model directory path\n",
    "pruned_model_path = '../../../image_classification/experiments_outputs/pruned_models/PR_L2_0.5'\n",
    "\n",
    "# Run recommendation compression\n",
    "compression_result = compressor.recommendation_compression(\n",
    "    compression_method=CompressionMethod.PR_L2,\n",
    "    recommendation_method=RecommendationMethod.SLAMP,\n",
    "    recommendation_ratio=0.5,\n",
    "    input_model_path=baseline_model_path,\n",
    "    output_dir=pruned_model_path,\n",
    "    framework=Framework.TENSORFLOW_KERAS,\n",
    "    input_shapes=[{\"batch\": 1, \"channel\": 3, \"dimension\": [224, 224]}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Fine-Tuning\">\n",
    "    <h2>4. Fine-Tuning the Pruned Model</h2>\n",
    "</div>\n",
    "\n",
    "Fine-tuning is a crucial step to address the slight reduction in accuracy that can occur in pruned models. This involves adjusting the remaining filters by re-training the model on the original or a similar dataset. By optimizing the model's performance through fine-tuning, the accuracy of the pruned model can be restored to match or even exceed that of the original model.\n",
    "\n",
    "To fine-tune  the pruned model generated by NetsPresso Compressor, you will need to utilize `stm32ai_main.py` and the `fine_tuning_config.yaml` file. This configuration file enables you to tailor the fine-tuning process by adjusting parameters such as `frozen_layer`, `epochs`, and `learning_rate`. It's crucial to select these parameters thoughtfully since they can significantly influence the model's performance. Additionally, it's essential to ensure that you use the same `global_seed`, `seed`, and `validation_split` in `dataset` section of the yaml file as you did during the training of the baseline model. This precautionary measure will prevent any leakage of training data into the validation. Also, it's important to ensure that `model_path` matches `pruned_model_path`, where the compressed version of the model will be saved, so that both paths should point to the same local directory.\n",
    "\n",
    "After fine-tuning, you can find the model saved under the **experiments_outputs/pruned_fine_tuning/best_model** folder, along with the training curves, confusion matrices, and log file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path  ../../../image_classification/config_files  --config-name fine_tuning_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Quantizing\">\n",
    "    <h2>5. Quantizing the Pruned Model</h2>\n",
    "</div>\n",
    "\n",
    "In this section, we will quantize the pruned float32 model to an int8 quantized tflite model. Quantization is a technique used to reduce the memory and computation requirements of a model by converting the weights and activations from float32 to int8.\n",
    "\n",
    "To perform quantization, you will need to use the configuration file `quantization_config.yaml` along with the `stm32ai_main.py` script. The configuration file specifies the `quantization_dataset_path` and the quantization parameters, such as the `quantization_input_type` and `quantization_output_type`. \n",
    "\n",
    "After running the `stm32ai_main.py` script with the `quantization_config.yaml` file, an int8 quantized tflite model will be generate and saved under **experiments_outputs/pruned_quantization/quantized_models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path  ../../../image_classification/config_files  --config-name quantization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Evaluating\">\n",
    "    <h2>6. Evaluating the Pruned Model</h2>\n",
    "</div>\n",
    "\n",
    "At this stage of the notebook, we have the model quantized and pruned. By running the code section below we can evaluate it after updating the `evaluation_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path  ../../../image_classification/config_files  --config-name evaluation_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Benchmarking\">\n",
    "        <h2>7. Benchmarking the Pruned Model on STM32Cube.AI Developer Cloud</h2>\n",
    "</div>\n",
    "\n",
    "After analyzing the effect of NetsPresso on accuracy, we will use the [STM32Cube.AI Developer Cloud](https://stm32ai-cs.st.com/home) in this section to compare the performance of the compressed neural network with the original model. We will benchmark the model on the **STM32H747I-DISCO** target, as we did with the baseline model, to determine the performance and verify the effectiveness of the NetsPresso Compressor on the MobileNet v2 model.\n",
    "\n",
    "To set the configuration parameters, we need to update the `pruned_benchmarking_config.yaml` file to specify the path of the quantized pruned model and the name of the `board`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path  ../../../image_classification/config_files  --config-name pruned_benchmarking_config.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Comparing\">\n",
    "        <h2> 8. Comparing NetsPresso Pruned Model with Baseline Model</h2>\n",
    "</div>\n",
    "\n",
    "This last section compares the performance of the baseline model with the NetsPresso compressed model by presenting a comparison table.\n",
    "By executing the final two sections, a table will be displayed that compares the quantized baseline model with the pruned quantized model using NetsPresso. The table will show the differences in terms of inference time, validation accuracy, RAM, and flash memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Change current working directory to \n",
    "os.chdir('../../../')\n",
    "\n",
    "sys.path.append(os.path.relpath('utils'))\n",
    "from utils import parse_logs_and_display_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for the log files  \n",
    "base_path = 'image_classification/experiments_outputs/'\n",
    "baseline_training = os.path.relpath(base_path + 'baseline_training/stm32ai_main.log')\n",
    "baseline_benchmarking = os.path.relpath(base_path + 'baseline_benchmarking/stm32ai_main.log')\n",
    "baseline_evaluation = os.path.relpath(base_path + 'pruned_evaluation/stm32ai_main.log')\n",
    "pruned_benchmarking = os.path.relpath(base_path + 'pruned_benchmarking/stm32ai_main.log')\n",
    "\n",
    "# Call function to parse logs and display results in a table \n",
    "parse_logs_and_display_results(baseline_training, baseline_benchmarking, baseline_evaluation, pruned_benchmarking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
